# ScaleDownâ„¢ Technical Specification

## Context
Standard LLMs and medical databases return vast amounts of text, leading to high token costs and "context window fatigue" for researchers. ScaleDown is a proprietary heuristic engine designed to solve this.

## Algorithm Overview
The compression process follows four distinct phases:

1. **Medical Tokenization**: Sentences are split using `natural` tokenizers and cross-referenced with a medical keyword library.
2. **Density Scoring**:
   - **Entity Boost**: +4 for keywords like *mortality*, *efficacy*, *cohort*.
   - **Data Boost**: +6 for statistical patterns (e.g., *p < 0.05*, *95% CI*).
   - **Structure Boost**: +5 for Conclusion/Conclusion-like sentences.
3. **PICO Retention**: Prioritizes sentences describing **P**opulation, **I**ntervention, **C**omparison, and **O**utcome.
4. **Context Preservation**: Selects the top-scoring sentences and re-orders them chronologically to maintain narrative flow.

## Key Performance Indicators (KPIs)
- **Compression Ratio**: 70-80% reduction in token count.
- **Accuracy Preservation**: ~90% retention of critical methodologies.
- **Cost Reduction**: Simulated 65% reduction in downstream LLM inference costs.
- **Concurrency**: Enables analysis of 10+ papers simultaneously vs 2-3 without compression.

## Implementation logic
Located in `backend/services/nlp.js`.
